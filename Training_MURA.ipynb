{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuraDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self,text_file,root_dir, transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                                                          transforms.ToTensor(),\n",
    "                                                                          transforms.Normalize(\n",
    "                                                                              mean=[0.456],\n",
    "                                                                              std= [0.225])])): #Normalize to pretrained VGGNet\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_file(string): path to text file\n",
    "            root_dir(string): directory with all train images\n",
    "        \"\"\"\n",
    "        #File with the Path\n",
    "        self.name_frame = pd.read_csv(text_file,sep=\",\",usecols=[0],dtype = 'str',nrows = 1000)\n",
    "        #File with labels\n",
    "        self.label_frame = pd.read_csv(text_file,sep=\",\", usecols=[1], nrows = 1000)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "                                       \n",
    "    def __len__(self):\n",
    "        return len(self.name_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Pull image\n",
    "        img_name = os.path.join(self.root_dir, self.name_frame.iloc[idx, 0])\n",
    "        \n",
    "        #Apply adapative threshold to highlight key features in the image   \n",
    "        image = cv2.imread(img_name,0)\n",
    "        image = cv2.adaptiveThreshold(image,255,cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY,11,2)  \n",
    "        image = Image.fromarray(image)\n",
    "        image = image.convert('L')\n",
    "        \n",
    "        #Apply tensor transformations to images\n",
    "        image = self.transform(image) \n",
    "        \n",
    "        # I included labels in the train_image_path file in excel using a find(\"positive\")\n",
    "        labels = self.label_frame.iloc[idx,0] \n",
    "        \n",
    "        #Formatting for labels\n",
    "        labels = np.array(labels)\n",
    "        labels = np.reshape(labels, (1,1))\n",
    "        labels= torch.from_numpy(labels.astype('int'))\n",
    "\n",
    "        sample = {'image': image, 'labels': labels}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MuraTrainSet = MuraDataset(text_file ='/Users/JosephVele/MURA-v1.1/Shoulder_Mura.csv',\n",
    "                                   root_dir = '/Users/JosephVele')\n",
    "\n",
    "MuraTrainLoader = torch.utils.data.DataLoader(MuraTrainSet,batch_size=100,shuffle=True, num_workers=0)\n",
    "\n",
    "MuraTestSet = MuraDataset(text_file ='/Users/JosephVele/MURA-v1.1/Shoulder_Mura_test.csv',\n",
    "                                   root_dir = '/Users/JosephVele')\n",
    "\n",
    "MuraTestLoader = torch.utils.data.DataLoader(MuraTestSet,batch_size=30,shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate mean &std - batch was sent to 8379 to match number of records in one batch\n",
    "\n",
    "for i_batch,sample_batched in enumerate(MuraTrainLoader,0):\n",
    "    numpy_image = sample_batched['image'].numpy()\n",
    "    image_mean = np.mean(numpy_image, axis=(0,2,3))\n",
    "    image_std = np.std(numpy_image, axis=(0,2,3))\n",
    "\n",
    "\n",
    "#image_mean=np.mean(x, axis= (0))\n",
    "#image_std=np.std(x, axis=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02527229]\n",
      "[0.96729124]\n"
     ]
    }
   ],
   "source": [
    "#Confirm images have been normalized appropriately with 0 mean and 1 std\n",
    "#This will help with vanishing gradient problem\n",
    "print(image_mean)\n",
    "print(image_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n",
      "Downloading: \"https://download.pytorch.org/models/densenet169-b2777c0a.pth\" to /Users/JosephVele/.torch/models/densenet169-b2777c0a.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "dense_net169 = models.densenet169(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1664])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for param in dense_net169.features.parameters():\n",
    "    param.requires_grad = False\n",
    "#Adjust to grayscale = 1 channel. Requires gradient is True by default\n",
    "dense_net169.features._modules['conv0']  = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "dense_net169.classifier = nn.Linear(in_features=1664, out_features=1, bias=True)\n",
    "#dense_net169\n",
    "\n",
    "for param in dense_net169.classifier.parameters():\n",
    "    print(param.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyper-parameters\n",
    "#******************************#\n",
    "\n",
    "learning = 1e-3 # Learning Rate \n",
    "moment =.9 #Momentum \n",
    "w = 1e-2\n",
    "b = [.9,999]\n",
    "#******************************#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Define criterion and optimizer\n",
    "#************************************#\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = optim.Adam(dense_net169.classifier.parameters(), lr=learning, weight_decay = w)\n",
    "\n",
    "#************************************#\n",
    "\n",
    "# 4.2 Train the model\n",
    "# 4.3 Please store and print training and validation loss&accuracy after each epoch\n",
    "#********************************************#\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "acc_data_train=[]\n",
    "acc_data_test=[]\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    dense_net169.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for i, sample_batched in enumerate(MuraTrainLoader,1):\n",
    "        inputs = sample_batched['image']\n",
    "        labels = sample_batched['labels']\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = dense_net169(inputs)\n",
    "        labels = labels.view(len(sample_batched['labels']),-1)\n",
    "        loss = criterion(outputs.float(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss+= loss.item()\n",
    "        \n",
    "        pred = (torch.sigmoid(outputs).data > 0.5).int()\n",
    "        labels = labels.int()\n",
    "        correct += pred.eq(labels.data.view_as(pred)).sum()\n",
    "\n",
    "\n",
    "    \n",
    "    #Print Results from first Epoch\n",
    "    #Accuracy\n",
    "    acc =  100.00 * float(correct) / float(len(MuraTrainLoader.dataset))\n",
    "    acc_data_train.append(acc)\n",
    "    #Average Loss\n",
    "    train_loss=float(train_loss)/float(i)\n",
    "    train_losses.append(train_loss)\n",
    "    # print statistics        \n",
    "    print('Train Epoch:{} Loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(epoch,\n",
    "            train_loss, correct, len(MuraTrainLoader.dataset), acc))\n",
    "\n",
    "def test(epoch):\n",
    "    #Have our model in evaluation mode\n",
    "    dense_net169.eval()\n",
    "    #Set losses and Correct labels to zero\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, sample_batched in enumerate(MuraTestLoader,1):\n",
    "            inputs = sample_batched['image']\n",
    "            labels = sample_batched['labels']\n",
    "            outputs = dense_net169(inputs)\n",
    "            labels = labels.view(len(sample_batched['labels']),-1)\n",
    "            loss = criterion(outputs.float(), labels.float())\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            pred = (torch.sigmoid(outputs).data > 0.5).int()\n",
    "            labels = labels.int()\n",
    "            correct += pred.eq(labels.data.view_as(pred)).sum()\n",
    "\n",
    "        test_loss = test_loss/i\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        acc =  100.00 * float(correct) / float(len(MuraTestLoader.dataset))\n",
    "        acc_data_test.append(acc)\n",
    "        print('Test Epoch:{} Loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(epoch,\n",
    "            test_loss, correct, len(MuraTestLoader.dataset),\n",
    "            acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:1 Loss: 0.6220, Accuracy: 662/1000 (66.20%)\n",
      "\n",
      "Test Epoch:1 Loss: 0.6629, Accuracy: 347/562 (61.74%)\n",
      "\n",
      "Train Epoch:2 Loss: 0.6089, Accuracy: 697/1000 (69.70%)\n",
      "\n",
      "Test Epoch:2 Loss: 0.6609, Accuracy: 344/562 (61.21%)\n",
      "\n",
      "Train Epoch:3 Loss: 0.6054, Accuracy: 696/1000 (69.60%)\n",
      "\n",
      "Test Epoch:3 Loss: 0.6671, Accuracy: 340/562 (60.50%)\n",
      "\n",
      "Train Epoch:4 Loss: 0.5878, Accuracy: 704/1000 (70.40%)\n",
      "\n",
      "Test Epoch:4 Loss: 0.6670, Accuracy: 332/562 (59.07%)\n",
      "\n",
      "Train Epoch:5 Loss: 0.5859, Accuracy: 710/1000 (71.00%)\n",
      "\n",
      "Test Epoch:5 Loss: 0.6675, Accuracy: 333/562 (59.25%)\n",
      "\n",
      "Train Epoch:6 Loss: 0.5757, Accuracy: 729/1000 (72.90%)\n",
      "\n",
      "Test Epoch:6 Loss: 0.6538, Accuracy: 354/562 (62.99%)\n",
      "\n",
      "Train Epoch:7 Loss: 0.5683, Accuracy: 744/1000 (74.40%)\n",
      "\n",
      "Test Epoch:7 Loss: 0.6575, Accuracy: 351/562 (62.46%)\n",
      "\n",
      "Train Epoch:8 Loss: 0.5651, Accuracy: 736/1000 (73.60%)\n",
      "\n",
      "Test Epoch:8 Loss: 0.6579, Accuracy: 353/562 (62.81%)\n",
      "\n",
      "Train Epoch:9 Loss: 0.5553, Accuracy: 740/1000 (74.00%)\n",
      "\n",
      "Test Epoch:9 Loss: 0.6541, Accuracy: 360/562 (64.06%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train + Test per epoch\n",
    "for epoch in range(1,10): \n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not Used - Originally constructing from scratch\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "#class ConvNet(nn.Module):\n",
    "#    def __init__(self, num_classes=2, drop1 = .1, drop2 = .5):\n",
    "#        super(ConvNet, self).__init__()\n",
    "#        # 3.1 Initialization\n",
    "#        #******************************#\n",
    "#        # 1 input image channel, 10 output channels, 1x1 square convolution\n",
    "#        # kernel\n",
    "#        self.conv1 = nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2)        \n",
    "#        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2) \n",
    "#        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)  \n",
    "#        self.conv4 =  nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
    "#        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "#              \n",
    "#        self.dense1_drop = nn.Dropout()   \n",
    "#        self.dense1 = nn.Linear(256 * 6 * 6, 4096)\n",
    "#        self.dense2_drop = nn.Dropout()\n",
    "#        self.dense2 = nn.Linear(4096, 4096)\n",
    "#        self.dense3 = nn.Linear(4096, num_classes-1)\n",
    "#                \n",
    "        \n",
    "        #******************************#\n",
    "#    def forward(self, x):\n",
    "        # 3.2 Define Neural Network\n",
    "        #******************************#\n",
    "        \n",
    "        #convolutional layers\n",
    "#        x = F.max_pool2d(F.relu(self.conv1(x), inplace=True),kernel_size=3, stride=2 )\n",
    "#        x = F.max_pool2d(F.relu(self.conv2(x), inplace=True),kernel_size=3, stride=2 )\n",
    "#        x = F.relu(self.conv3(x), inplace=True)\n",
    "#        x = F.relu(self.conv4(x), inplace=True)\n",
    "#        x = F.max_pool2d(F.relu(self.conv5(x), inplace=True),kernel_size=3, stride=2 )\n",
    "        \n",
    "        #flatten\n",
    "#        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        \n",
    "        #dense layers\n",
    "#        x = F.relu(self.dense1(self.dense1_drop(x)))\n",
    "#        x = F.relu(self.dense2(self.dense2_drop(x)))\n",
    "        \n",
    "#        x = self.dense3(x)\n",
    "        \n",
    "        #******************************#\n",
    "#        return x\n",
    "        \n",
    "                       \n",
    "#model = ConvNet(num_classes=2).to(device)\n",
    "# Note: what is the difference between 'same' and 'valid' padding? \n",
    "# Take a look at the outputs to understand the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tested initially but didn't work\n",
    "#for param in vgg_19_bn.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "#vgg_19_bn.features._modules['0'] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "#vgg_19_bn.classifier._modules['7'] = vgg_19_bn.classifier._modules['4']\n",
    "#vgg_19_bn.classifier._modules['8'] = nn.Dropout(p=0.5)\n",
    "#vgg_19_bn.classifier._modules['9'] = nn.Linear(1000, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
